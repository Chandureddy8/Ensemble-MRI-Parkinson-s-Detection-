# -*- coding: utf-8 -*-
"""parkinson's detection using CNN & Transfer learning .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q_rNsCQJ1yIdOEJDaIIJJ0W0J3stgrv_

**Using Individual Models**
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import matplotlib.pyplot as plt
import numpy as np

"""**Load Images**"""

images = "/content/drive/MyDrive/parkinsons t2/train"

folders = os.listdir(images)
print(folders)

image_data= []
labels = []

label_dict = {
    'control':0,
    'pd':1
}

from keras.preprocessing import image

for ix in folders:
  path = os.path.join(images,ix)
  for im in os.listdir(path):
    img = image.load_img(os.path.join(path,im),target_size=((512,512)))
    img_array = image.img_to_array(img)
    image_data.append(img_array)
    labels.append(label_dict[ix])

print(len(image_data),len(labels))

combined = list(zip(image_data,labels))
image_data[:],labels[:] = zip(*combined)

print(labels)

x_train = np.array(image_data)
y_train = np.array(labels)

print(x_train.shape,y_train.shape)

from keras.utils import np_utils

y_train = np_utils.to_categorical(y_train)
print(x_train.shape,y_train.shape)

from keras.preprocessing.image import ImageDataGenerator

"""**Data Augmentation**"""

augment = ImageDataGenerator( 
                             rotation_range=20,
                              width_shift_range=0.01, 
                              height_shift_range=0.01, 
                              horizontal_flip=False, 
                              vertical_flip=False,
                            )
augment.fit(x_train)

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import *
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.regularizers import l2

"""**CNN_Model**"""

model = Sequential()


model.add(Conv2D(filters=16, kernel_size=(3,3),input_shape=(512,512,3), activation='relu',kernel_regularizer=l2(0.01)))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(512,512,3), activation='relu',kernel_regularizer=l2(0.01)))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=(512,512,3), activation='relu',kernel_regularizer=l2(0.01)))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=(512,512,3), activation='relu',kernel_regularizer=l2(0.01)))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(filters=128, kernel_size=(3,3),input_shape=(512,512,3), activation='relu',kernel_regularizer=l2(0.01)))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(filters=128, kernel_size=(3,3),input_shape=(512,512,3), activation='relu',kernel_regularizer=l2(0.01)))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Flatten())

model.add(Dense(224))
model.add(Activation('relu'))

model.add(Dropout(0.5))

model.add(Dense(2))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
filepath="parkinsons_detection.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min',patience=3)
callbacks_list = [checkpoint]

model.summary()

hist = model.fit(x_train,y_train,
                    shuffle = True,
                    batch_size=32,
                    epochs = 25,
                    validation_split = 0.10,callbacks=callbacks_list)

plt.figure(1, figsize = (15, 5))
plt.subplot(1,2,1)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.plot( hist.history["loss"], label = "Training Loss")
plt.plot( hist.history["val_loss"], label = "Validation Loss")
plt.grid(True)
plt.legend()

plt.subplot(1,2,2)
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.plot( hist.history["accuracy"], label = "Training Accuracy")
plt.plot( hist.history["val_accuracy"], label = "Validation Accuracy")
plt.grid(True)
plt.legend()

"""**Testing CNN-Model performance**"""

test_images = "/content/drive/MyDrive/parkinsons t2/test"

test_image_data = []
test_labels = []

test_folders = os.listdir(test_images)
print(test_folders)

label_dict = {
    'control':0,
    'pd':1
}

from keras.preprocessing import image

for ix in test_folders:
    path = os.path.join(test_images,ix)
    for im in os.listdir(path):
        img = image.load_img(os.path.join(path,im),target_size = ((512,512)))
        img_array = image.img_to_array(img)
        test_image_data.append(img_array)
        test_labels.append(label_dict[ix])
        

combined = list(zip(test_image_data,test_labels))
test_image_data[:],test_labels[:] = zip(*combined)

x_test = np.array(test_image_data)
y_test = np.array(test_labels)

from keras.utils import np_utils

y_test = np_utils.to_categorical(y_test)
print(x_test.shape,y_test.shape)

model.evaluate(x_test,y_test)

from sklearn.metrics import classification_report,confusion_matrix

predictions = model.predict(x_test, batch_size = 32)
pred = np.argmax(predictions, axis=1)

print(classification_report(test_labels, pred))

print(confusion_matrix(test_labels, pred))

from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model

"""**VGG16-Model**"""

model_vgg = VGG16(include_top = False,weights = 'imagenet',input_shape = (512,512,3))

model_vgg.summary()

for ix in range(len(model_vgg.layers)):
    print(ix,model_vgg.layers[ix])

for layer in model_vgg.layers[:15]:
    layer.trainable = False
for i, layer in enumerate(model_vgg.layers):
    print(i, layer.name, layer.trainable)
adam = Adam(learning_rate=0.00003)
model_vgg.compile(loss='binary_crossentropy',optimizer = adam,metrics=['accuracy'])
model_vgg.summary()

av1 = Flatten()(model_vgg.output)
fc1 = Dense(256,activation='relu',kernel_regularizer= l2(0.01),input_dim=256)(av1)
d1 = Dropout(0.5)(fc1)
fc2 = Dense(128,activation='relu',kernel_regularizer= l2(0.01),input_dim=128)(d1)
d2 = Dropout(0.5)(fc2)
fc3 = Dense(2,activation = 'sigmoid')(d2)


model_new = Model(model_vgg.input,fc3)
model_new.summary()

adam = Adam(learning_rate=0.00001)
model_new.compile(loss='binary_crossentropy',optimizer = adam,metrics=['accuracy'])

from tensorflow.keras.callbacks import ModelCheckpoint

filepath="parkinsons_detection_vgg16.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min',patience=2)
callbacks_list = [checkpoint]

hist = model_new.fit(x_train,y_train,
                    shuffle = True,
                    batch_size=32,
                    epochs = 20,
                    validation_split = 0.10,callbacks=callbacks_list)

plt.figure(1, figsize = (15, 5))
plt.subplot(1,2,1)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.plot( hist.history["loss"], label = "Training Loss")
plt.plot( hist.history["val_loss"], label = "Validation Loss")
plt.grid(True)
plt.legend()

plt.subplot(1,2,2)
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.plot( hist.history["accuracy"], label = "Training Accuracy")
plt.plot( hist.history["val_accuracy"], label = "Validation Accuracy")
plt.grid(True)
plt.legend()

"""**Testing_vgg16 performance**"""

model_new.evaluate(x_test,y_test)

predictions = model_new.predict(x_test, batch_size = 32)
pred = np.argmax(predictions, axis=1)

print(classification_report(test_labels, pred))

print(confusion_matrix(test_labels, pred))

from tensorflow.keras.applications.resnet50 import ResNet50

"""**Resnet50-model**"""

model = ResNet50(include_top = False,weights = 'imagenet',input_shape = (512,512,3))

model.summary()

for ix in range(len(model.layers)):
    print(ix,model.layers[ix])

for layer in model.layers[:169]:
    layer.trainable = False
for i, layer in enumerate(model.layers):
    print(i, layer.name, layer.trainable)
adam = Adam(learning_rate=0.00003)
model.compile(loss='binary_crossentropy',optimizer = adam,metrics=['accuracy'])
model.summary()

av1 = Flatten()(model.output)
fc1 = Dense(256,activation='relu',kernel_regularizer= l2(0.01),input_dim=256)(av1)
d1 = Dropout(0.5)(fc1)
fc2 = Dense(128,activation='relu',kernel_regularizer= l2(0.01),input_dim=128)(d1)
d2 = Dropout(0.5)(fc2)
fc3 = Dense(2,activation = 'sigmoid')(d2)


model_resnet = Model(model.input,fc3)
model_resnet.summary()

adam = Adam(learning_rate=0.00001)
model_resnet.compile(loss='binary_crossentropy',optimizer = adam,metrics=['accuracy'])

filepath="parkinsons_detection_resnet50.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min',patience=2)
callbacks_list = [checkpoint]

hist = model_resnet.fit(x_train,y_train,
                    shuffle = True,
                    batch_size=32,
                    epochs = 20,
                    validation_split = 0.10,callbacks=callbacks_list)

plt.figure(1, figsize = (15, 5))
plt.subplot(1,2,1)
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.plot( hist.history["loss"], label = "Training Loss")
plt.plot( hist.history["val_loss"], label = "Validation Loss")
plt.grid(True)
plt.legend()

plt.subplot(1,2,2)
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.plot( hist.history["accuracy"], label = "Training Accuracy")
plt.plot( hist.history["val_accuracy"], label = "Validation Accuracy")
plt.grid(True)
plt.legend()

"""**Testing Resnet50 Model**"""

model_resnet.evaluate(x_test,y_test)

predictions = model_resnet.predict(x_test, batch_size = 32)
pred = np.argmax(predictions, axis=1)

print(classification_report(test_labels, pred))

print(confusion_matrix(test_labels, pred))

